{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All about loading, cleaning and summarising data in Python\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook we are going to load, clean and summarise different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files in Python\n",
    "\n",
    "To load data in Python you can use one of the most comprehensive libraries in Python called `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will help you to gain experience loading CSV, Excel, TSV, and JSON files. We are going to use `pandas` to perform basic exploration of the data.\n",
    "The CSV, TVS, and Excel files contain information about happiness index and alcohol consumption in the world. The files are not identical but they are very similar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the `happiness.csv` database\n",
    "This file contains information about country name and its respective region, hemisphere, Happiness score (*A metric measured in 2015 by asking the sampled people the question: \"How would you rate your happiness on a scale of 0 to 10 where 10 is the happiest.\"*), [Human Development Index](https://en.wikipedia.org/wiki/Human_Development_Index), GDP per capita, Litres (per capita) of beer consumption, Litres (per capita) of spirit consumption , Litres (per capita) of wine consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "happiness = pd.read_csv('happiness.csv')\n",
    "happiness.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the `happiness.xlsx` database\n",
    "\n",
    "`.xlsx` files are a bit more complex than `.csv` or `.tsv` files. When you are going to load an Excel file you need to specify the spread-sheet name, by default the function `read_excel` is going to read the first spread-sheet. In this case we are going to create a new variable with the location, this is not necessary but it can help if you want to scale or share your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what is happening when the spread-sheet name is not specified\n",
    "Location = 'happiness.xlsx'\n",
    "df_xlsx = pd.read_excel(Location)\n",
    "# let's observe the first five entries\n",
    "df_xlsx.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem that it is the spread-sheet that we need, right?\n",
    "Let's try again but this time let's specify the spread-sheet name to `happiness`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what is happening when the spread-sheet name is specified\n",
    "Location = 'happiness.xlsx'\n",
    "# Remember that `Location` was defined above\n",
    "df_xlsx = pd.read_excel(Location, sheet_name = 'happiness') \n",
    "# Let's observe the last seven entries\n",
    "df_xlsx.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the `happiness.tsv` database\n",
    "\n",
    "`.tsv` files are relatively easy to read, some people can store similar information in `.txt` files as well. The important part to read these files is to know what symbol is separating the columns of the dataset. For `.tsv` files the separating column symbol is `tab`, but in a `.txt` can be `;`, `:` or `,`. We need to specify that in the parameter `sep`, for `tab` the separate value is `'\\t'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv = pd.read_table('happiness.tsv', sep = '\\t')\n",
    "df_tsv.head() # The default values in python for the function head() is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the `grades.json` database\n",
    "\n",
    "`.json` files are machine/human readable files, they are very common especially if we are importing data from databases online. It is important to specify that we want to have is a data frame as a result, so in that case the parameter `orient` should be `table`. In this case the file `grades.json` correspond to a database with names of students and their grade result at the end of the academic year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = pd.read_json('grades.json', orient = 'table')\n",
    "df_json.tail(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme challenge - Loading GAP minder data\n",
    "We do not necessarily need to download data into our computers first and then load the file into Python. Sometimes we can load the data online which simplifies any project collaboration because there is only one single version of the data! With that we do not need to share the code and the files, we just need to share the code! Your challenge now is to load data of Total 25-54 unemployment (%) per country from 1981 to 2005. Check out the GAP minder website [here](www.gapminder.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data from GAP minder foundation\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://docs.google.com/spreadsheet/pub?key=rEMA-cbNPaOtpDyxTcwugnw&output=xlsx'\n",
    "\n",
    "# Read in all sheets of Excel file: excel_file\n",
    "excel_file =  pd.read_excel(url, sheet_name = None)\n",
    "\n",
    "# Save the sheetnames to spread_sheets_names\n",
    "spread_sheets_names = excel_file.keys()\n",
    "\n",
    "# Save the Data sheet into heck the head of the  \n",
    "gapminder_data = excel_file['Data']\n",
    "gapminder_data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "In this module we are going to clean two datasets. The first dataset is the happiness score database per country with information about alcohol consumption, GDP per capita abad Human Development Index. The second is the GAP minder dataset of Total 25-54 unemployment (%). In both datasets we are going to face different challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check again the head of happiness\n",
    "happiness.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing columns names\n",
    "\n",
    "As you can see the names of the variables are not very informative, maybe we can rename the columns with more informative names. To do that we need to use the function `.rename()` using the argument `columns` and `inplace`. Column names like: 'Country', 'Region', 'Hemisphere', 'HappinessScore', 'HDI', 'GDP_PerCapita', 'Beer_PerCapita', 'Spirit_PerCapita', 'Wine_PerCapita' will be more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the current columns names and create a variable with the new names\n",
    "oldnames = happiness.columns\n",
    "newnames = ['Country', 'Region', 'Hemisphere', 'HappinessScore',\n",
    "            'HDI', 'GDP_PerCapita', 'Beer_PerCapita', \n",
    "            'Spirit_PerCapita', 'Wine_PerCapita']\n",
    "# Let's make a new dictionary with the old and new column names.\n",
    "changeCol = dict(zip(oldnames,newnames))\n",
    "changeCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename the columns we need to specify in the argument columns the dictionary.\n",
    "# This dictionary contains the current names of the columns as keys,\n",
    "# and the new columns as values\n",
    "# Inplace = True will overwrite the file\n",
    "happiness.rename(columns = changeCol, inplace = True )\n",
    "# Let's check if the column names changed\n",
    "happiness.columns \n",
    "# if you do not want to overwrite your file and create a new one\n",
    "# you need to specify that using\n",
    "# new_file = happiness.rename(columns = changeCol) \n",
    "# but in this case you have to work with new_file and not with happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the type of the variables\n",
    "\n",
    "This dataset also contains a very common error, the decimal symbol of the columns 'HappinessScore' and 'GDP_PerCapita' is a comma and not a point. In some countries the comma is a valid decimal symbol ??!\n",
    "\n",
    "However, Python will understand that 2,34 is a series of characters '2,34' and not a number such as 2.43.\n",
    "\n",
    "For that reason we need to change the commas to points and then transform the `str` data type to a numeric data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check what are the type of data for each variable in `happiness`\n",
    "# using the function `dtypes`\n",
    "happiness.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see some variables that are supposed to be numeric are objects, in other words Python think these variables are words and not numbers!\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "You can select a single variable using the following notations\n",
    "\n",
    "1. dataframe['name_of_the_column']\n",
    "2. dataframe.name_of_the_column\n",
    "\n",
    "We are going to talk more about that and in more detail during the challenge 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this step we are going to change the commas for points \n",
    "happiness['HappinessScore'] = happiness['HappinessScore'].apply(lambda x: x.replace(',', '.'))\n",
    "happiness['GDP_PerCapita'] = happiness['GDP_PerCapita'].apply(lambda x: x.replace(',', '.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now the only thing that we did was changing a comma for a point but the type of variable is still a series of characters. Let's tell Python that actually these valuaes are not series of characters but a number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the type of the variables\n",
    "happiness.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to change the datatype we are going to apply  the function .astype()\n",
    "## to the column that we want to transform and rewrite that\n",
    "happiness['HappinessScore'] = happiness['HappinessScore'].astype(float)\n",
    "happiness['GDP_PerCapita'] = happiness['GDP_PerCapita'].astype(float)\n",
    "# Let's check the data types\n",
    "happiness.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the last three entries\n",
    "happiness.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors in `str` values\n",
    "Cool! Now let's check whether there are errors with the names of the variables. In this case we are going to focus on the variable `Hemisphere` which only can have three possible values `north`, `south` or `both`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check all the possible values of the column `Hemisphere`\n",
    "happiness.Hemisphere.unique()\n",
    "# Another way to do the same is happiness['Hemisphere'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here there are more than three values, some entries contains typos and others  use a single letter either capitalised or not. We need to change that to have only three possible values, and it is very easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace the values that contain mistakes and then rewrite in our original file\n",
    "happiness.Hemisphere = happiness.Hemisphere.replace(['North', 'N', 'noth', 'nort', 'n'],'north')\n",
    "happiness.Hemisphere = happiness.Hemisphere.replace(['s','S', 'South', 'sout', 'southh'], 'south')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check all the possible values of the column `Hemisphere`\n",
    "happiness.Hemisphere.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now the dataset is cleaned!**\n",
    "## Cleaning the `gapminder_data` dataset\n",
    "## Pivot the table\n",
    "Usually a structured data should have atributes or variables in the columns and observations for each row. The `gapminder_data` do not have that format, in this case each country seems to be an observation, where there is a whole variable goin on called `Total 25-54 unemployment (%)` which is defined by year. That is not enterily correct because each column should represent a unique variable, therefore the variable 'year' do not have a unique column. Indeed what we have is a dataset which is pivoted ??\n",
    "\n",
    "For that reason we need to \"unpivot\" the table so each observation will contain the following variables: Country name, Year, and Total 25-54 unemployment (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look again to the head of gapminder_data\n",
    "gapminder_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the column names of gapminder_data\n",
    "gapminder_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's change the name of the variable 'Total 25-54 unemployment (%)' to 'Country'\n",
    "gapminder_data.rename(columns = {'Total 25-54 unemployment (%)':'COUNTRY'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the column names of gapminder_data\n",
    "gapminder_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to \"unpivot\" a table we are going to use the method `.melt()`.\n",
    "One of the key parameter is `id_vars`, we are going to definde the `id_vars` = `COUNTRY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt the gapminder_data variable\n",
    "gmd_melt = pd.melt(gapminder_data, id_vars= ['COUNTRY'])\n",
    "gmd_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see now each observation is defined by three variables: `COUNTRY`, `variable` and `value`. Now we need to rename these variables to a more informative name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to rename columns\n",
    "gmd_melt = gmd_melt.rename(columns={'variable':'year','value':'25_54_unemployment'})\n",
    "gmd_melt.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmd_melt.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set year variable as numeric\n",
    "gmd_melt['year'] = pd.to_numeric(gmd_melt['year'])\n",
    "# check to see if dataframe is properly melted\n",
    "gmd_melt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmd_melt.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the dimention of our database\n",
    "gmd_melt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NaN` values! ??????\n",
    "`NaN` = not a number\n",
    "\n",
    "Sometimes when we are dealing with dataset we found that some observations do not contain values. Either because the information was not available at that moment or because the file was corrupted. \n",
    "\n",
    "As you can see in `gmd_melt` there are some observations (country and year) of which we do not know the total 25-54 unemployment percentage. In this case we are going to eliminate that value. In other words, we are going to drop all the observations which contain `NaN` using the function `dropna()`. We can eliminate the whole column (in that case we will lose the data) or we could eliminate the rows. We can select that modifying the value of the parameter `axis`, 0 for rows and 1 for columns. \n",
    "\n",
    "Dealing with `NaN`s is the most common problem in data analysis. Sometimes we can replace the NaN value for the average or the value of the observation below or above. We can learn more about that reading these [article](https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all the NaN\n",
    "gdm_noNaN = gmd_melt.dropna(axis = 0)\n",
    "gdm_noNaN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmd_melt.shape)\n",
    "print(gdm_noNaN.shape)\n",
    "# Remember previously there were 725 observations and now we have 518\n",
    "# We dropped 207 values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have cleaned all the data!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data summary!\n",
    "In data summary we are going to calculate the measures of central tendency and central dispersion to understand the whole data. Finally we are going to use pivot tables to put in a nutshell the dataset!\n",
    "In this case we are going to use the `happiness` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many observations are? Let's count the countries\n",
    "totalCounts = happiness['Country'].count()  # number of values\n",
    "totalCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the total summation of all the HappinesScore values of the 122 countries\n",
    "summation = happiness['HappinessScore'].sum()  # summation of values all values\n",
    "summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To calculate the mean (or average) we are going to divide summation into totalCounts\n",
    "meanHappinessScore = summation / totalCounts\n",
    "meanHappinessScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements of central tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the function .mean() to calculate the average\n",
    "# You can use the function .median() to calculate the median; as well as .quantile(0.5)\n",
    "# You can use the function .mode() to calculate the mode, \n",
    "## remember that could be more than one mode, the .mode return\n",
    "# a dataframe with the most common values\n",
    "[mean, median, mode] = [happiness['HappinessScore'].mean(),\n",
    "                        happiness['HappinessScore'].median(),\n",
    "                        happiness['HappinessScore'].mode()]\n",
    "print('The mean is:', mean, ', the median is:', median,\n",
    "      ',and the mode is:', mode[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements of central dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the function .var() to calculate the average\n",
    "# You can use the function .std() to calculate the median\n",
    "# You can use the function .quantile() to calculate any quantile, \n",
    "## remember Q1 is the first 25% of the data and Q3 is the first 75% of the data\n",
    "[variance,standardDev, Q1, Q3] = [happiness['HappinessScore'].var(),\n",
    "                        happiness['HappinessScore'].std(),\n",
    "                        happiness['HappinessScore'].quantile(.25),\n",
    "                        happiness['HappinessScore'].quantile(.75)        ]\n",
    "print('The variance is:', variance, ', the standard deviation is:', standardDev,\n",
    "      ',and the interquartile range is: (',Q1,',',Q3,')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the maximum and minimum you need the functions .max and .min, respectively\n",
    "[minimum, maximum] = [happiness['HappinessScore'].min(),\n",
    "                    happiness['HappinessScore'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of all the numeric variables\n",
    "happiness.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the best functions in pandas! `.describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the whole dataset\n",
    "happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to pivot `happiness` so we are going to have a better understanding of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pivot the table we are going to use .pivot_table\n",
    "hemisphere = pd.pivot_table(happiness, index = 'Hemisphere')\n",
    "hemisphere\n",
    "#Have you spot any pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = pd.pivot_table(happiness, index = 'Region')\n",
    "region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We just finished to summirise the data analytically**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarise the data graphically ?? ?? ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to summirise the data visually! Remember a picture is worth a thousand words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first step is always to import the library for visualisation\n",
    "# in this case we are going to use matplotlib.pyplot and seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot of Country alcohol consumption for beer, spirits and wine \n",
    "sns.boxplot(data=happiness[['Beer_PerCapita','Spirit_PerCapita', 'Wine_PerCapita']])\n",
    "plt.title('Country Alcohol Consumption') # add a title\n",
    "plt.ylabel('Litres per capita (l)') # add a label for the y-axis\n",
    "plt.xticks(ticks = (0,1,2),labels =['Beer', 'Spirit', 'Wine'])  # add labels for the x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot of Happiness score by hemisphere\n",
    "sns.boxplot(y= 'Hemisphere', x = 'HappinessScore', data=happiness, \n",
    "            order = ['north', 'both', 'south'])\n",
    "plt.title('Country Happiness Score')\n",
    "plt.xlabel('Happiness Score')\n",
    "plt.ylabel('Hemisphere')\n",
    "plt.yticks(ticks = (0,1,2),labels =['North', 'Both', 'South'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violin plot of GDP_PerCapita\n",
    "sns.violinplot(x= 'Hemisphere', y = 'GDP_PerCapita',\n",
    "               order = ('north', 'both', 'south'), data=happiness)\n",
    "plt.title('Gross domestic product per capita')\n",
    "plt.ylabel('GDP per capita')\n",
    "plt.xlabel('Region')\n",
    "plt.xticks(ticks = (0,1,2),labels =('North', 'Both', 'South'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot between Wine_PerCapita and HappinessScore\n",
    "sns.scatterplot(x= 'Wine_PerCapita', y = 'HappinessScore', hue = 'Hemisphere',data=happiness)\n",
    "plt.title('Wine per capita and happiness')\n",
    "plt.ylabel('Happiness score')\n",
    "plt.xlabel('Litres of wine per capita (l)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of HappinessScore\n",
    "sns.distplot(happiness['HappinessScore'],kde=False, bins = 5)\n",
    "plt.title('Happiness Score Distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Happiness score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Density distribution of HappinessScore\n",
    "# the density curve allowed you to know the distribution of the data\n",
    "sns.distplot(happiness['HappinessScore'],hist=False,)\n",
    "plt.title('Happiness Score Distribution')\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Happiness score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extreme challenge!\n",
    "sns.pairplot(happiness, hue = 'Hemisphere')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to start filtering. Filtering data is a quintessential characteristic of data analysis. Recognising the different methods to filter is very important, in this section we are going to filter the data using the majority of methods to filter. Always remember that the method that you feel more comfortable with is the method that you should use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `happiness`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the countries which belong to both hemispheres\n",
    "# Method 1\n",
    "bothHemispheres = happiness[(happiness.Hemisphere=='both')]\n",
    "bothHemispheres.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the countries which belong to both hemispheres\n",
    "# Method 2 \n",
    "bothHemispheres = happiness[(happiness['Hemisphere']=='both')]\n",
    "bothHemispheres.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the countries which belong to both hemispheres\n",
    "# Method 3 using iloc\n",
    "bothHemispheres = happiness[(happiness.iloc[:,2]=='both')]\n",
    "bothHemispheres.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the countries which belong to both hemispheres\n",
    "# Method 4 using .loc\n",
    "bothHemispheres = happiness.loc[happiness['Hemisphere']=='both']\n",
    "bothHemispheres.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the countries that the litres of beer consumption per capita is higher than 150\n",
    "# Method 1\n",
    "beer150 =  happiness[(happiness.Beer_PerCapita > 150)]\n",
    "beer150.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the countries that the litres of beer consumption per capita is higher than 150\n",
    "# Method 2\n",
    "beer150 =  happiness[(happiness['Beer_PerCapita'] > 150)]\n",
    "beer150.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the countries that the litres of beer consumption per capita is higher than 150\n",
    "# Method 3 using iloc\n",
    "beer150 = happiness[(happiness.iloc[:,6]> 150)]\n",
    "beer150.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the countries that the litres of beer consumption per capita is higher than 150\n",
    "# Method 4 using iloc\n",
    "beer150 = happiness.loc[happiness['Beer_PerCapita']> 150]\n",
    "beer150.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset  of `happiness` that only includes from the second to the seventh column, \n",
    "# and the first 35 obs\n",
    "subset1 = happiness.iloc[:35, 1:7]\n",
    "subset1.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of `happiness` that includes only the third and sixth column, and values with `GDP_PerCapita` higher or equal to 40\n",
    "subset2_1 = happiness[happiness['GDP_PerCapita']>40]\n",
    "subset2 = subset2_1.iloc[:,[4,7]]\n",
    "subset2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gmd_melt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of gmd_melt that only contains `NaN` data\n",
    "gmd_NaN = gmd_melt[pd.isnull(gmd_melt['25_54_unemployment'])]\n",
    "gmd_NaN.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Now the data is loaded, cleaned, and summarised!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}